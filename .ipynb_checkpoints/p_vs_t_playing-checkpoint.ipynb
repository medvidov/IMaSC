{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import json\n",
    "import imblearn\n",
    "from utils import prodigy_to_spacy\n",
    "from metrics_clean import Metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PerformanceVsTraining:\n",
    "    def __init__(self, n:int = 20, verbose:bool = False, train_path: str = \"training_annotations.jsonl\", test_path: str = \"shaya_validate_test.jsonl\", label: list = ['INSTRUMENT', 'SPACECRAFT']) -> None:\n",
    "        #starters from parameters\n",
    "        self.train_path = train_path\n",
    "        self.train_file = None\n",
    "        self.test_path = test_path\n",
    "        self.test_file = None\n",
    "        self.num_data_points = n\n",
    "        self.anns_per_point = None\n",
    "        self.anns_this_round = 0 #changes with each round\n",
    "        self.label = label\n",
    "        self.metrics = Metrics(\"Baby\", \"shaya_validate_test.jsonl\")\n",
    "        self.t_vs_p = {}\n",
    "        self.nlp = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _reset_data(self) -> None:\n",
    "        #reset all metrics things between rounds\n",
    "        self.tp = 0.0\n",
    "        self.fp = 0.0\n",
    "        self.fn = 0.0\n",
    "        self.truths = set()\n",
    "        self.guesses = set()\n",
    "        self.num_truths = 0\n",
    "        self.accuracy = 0.0\n",
    "        self.recall = 0.0\n",
    "        self.f1 = 0.0\n",
    "        self.precision = 0.0\n",
    "        self.data_annotated = open(self.annotated_path)\n",
    "        self.data_raw = open(self.raw_path)\n",
    "\n",
    "    def _prep_data(self) -> None:\n",
    "        self.train_file = prodigy_to_spacy(self.train_path)\n",
    "        num_anns = sum(1 for item in self.train_file) #total number of annotations\n",
    "        self.train_file = prodigy_to_spacy(self.train_path)\n",
    "        self.anns_per_point = num_anns / self.num_data_points\n",
    "        self.test_file = prodigy_to_spacy(self.test_path)\n",
    "\n",
    "    def _run_metrics(self) -> int:\n",
    "        return self.metrics.calculate()\n",
    "\n",
    "\n",
    "    def _train_one_round(self, i: int) -> None:\n",
    "        n_iter = 100 #number of iterations. could make this customizable but I feel that it would be too messy\n",
    "        #train model and save to self.nlp\n",
    "        self.anns_this_round = i * self.anns_per_point\n",
    "        if self.verbose:\n",
    "            print(\"Training on %s annotations\" % (self.anns_this_round))\n",
    "        count = 0\n",
    "        train_data = []\n",
    "        for line in self.train_file:\n",
    "            train_data.append(line)\n",
    "            count += 1\n",
    "            if count >= self.anns_this_round:\n",
    "                break\n",
    "        \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "        random.seed(0)\n",
    "        self.nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        # Add entity recognizer to model if it's not in the pipeline\n",
    "        # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "        if \"ner\" not in self.nlp.pipe_names:\n",
    "            ner = self.nlp.create_pipe(\"ner\")\n",
    "            self.nlp.add_pipe(ner)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        else:\n",
    "            ner = self.nlp.get_pipe(\"ner\")\n",
    "\n",
    "        for label in self.label:\n",
    "            ner.add_label(label)  # add new entity label to entity recognizer\n",
    "        optimizer = self.nlp.begin_training()\n",
    "\n",
    "        move_names = list(ner.move_names)\n",
    "        # get names of other pipes to disable them during training\n",
    "        pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "        # only train NER\n",
    "        with self.nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "            # show warnings for misaligned entity spans once\n",
    "            warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "            sizes = compounding(1.0, 4.0, 1.001)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(train_data)\n",
    "                # Need some oversampling somewhere in here\n",
    "                batches = minibatch(train_data, size=sizes)\n",
    "                losses = {}\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    self.nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "                #print(\"Losses\", losses)\n",
    "        output_dir = Path(\"Baby\")\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        self.nlp.meta[\"name\"] = \"BabyModel\"  # rename model\n",
    "        self.nlp.to_disk(output_dir)\n",
    "\n",
    "    def run_test(self):\n",
    "        self._prep_data()\n",
    "        for i in tqdm(range(1, self.num_data_points + 1)):\n",
    "            self._train_one_round(i)\n",
    "            f1 = self._run_metrics()\n",
    "            self.t_vs_p[round(self.anns_this_round,3)] = round(f1, 3)\n",
    "            print(self.t_vs_p)\n",
    "\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "#     new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "#     n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    "# )\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"hello\")\n",
    "    p = PerformanceVsTraining(100, True)\n",
    "    p.run_test()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     plac.call(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
